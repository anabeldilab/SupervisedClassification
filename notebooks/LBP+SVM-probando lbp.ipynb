{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"executionInfo":{"elapsed":146322,"status":"error","timestamp":1710345740704,"user":{"displayName":"Anabel Díaz Labrador","userId":"11823624713775819332"},"user_tz":-60},"id":"dUCynuc_layU","outputId":"25de4a56-d620-447a-8218-437da10ce24c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Datos guardados en el archivo CSV.\n","Claves: ['radio', 'n_puntos', 'Exactitud', 'Sensibilidad', 'Especificidad', 'Precisión', 'F1-Score', 'AUC']\n","Valores: [2, 16, 72.43589743589743, 92.82051282051282, 38.46153846153847, 71.54150197628458, 0.8080357142857143, 0.8435349550734166]\n","Datos guardados en el archivo CSV.\n","Claves: ['radio', 'n_puntos', 'Exactitud', 'Sensibilidad', 'Especificidad', 'Precisión', 'F1-Score', 'AUC']\n","Valores: [3, 24, 71.7948717948718, 93.07692307692308, 36.324786324786324, 70.8984375, 0.8048780487804879, 0.8347030462415077]\n","Datos guardados en el archivo CSV.\n","Claves: ['radio', 'n_puntos', 'Exactitud', 'Sensibilidad', 'Especificidad', 'Precisión', 'F1-Score', 'AUC']\n","Valores: [4, 32, 71.31410256410257, 92.56410256410257, 35.8974358974359, 70.64579256360078, 0.8013318534961155, 0.8294104755643218]\n","Datos guardados en el archivo CSV.\n","Claves: ['radio', 'n_puntos', 'Exactitud', 'Sensibilidad', 'Especificidad', 'Precisión', 'F1-Score', 'AUC']\n","Valores: [2, 12, 72.27564102564102, 93.07692307692308, 37.60683760683761, 71.31630648330058, 0.8075639599555061, 0.8431952662721893]\n","Datos guardados en el archivo CSV.\n","Claves: ['radio', 'n_puntos', 'Exactitud', 'Sensibilidad', 'Especificidad', 'Precisión', 'F1-Score', 'AUC']\n","Valores: [2, 14, 71.47435897435898, 95.64102564102565, 31.196581196581196, 69.85018726591761, 0.8073593073593074, 0.8234494849879466]\n","Datos guardados en el archivo CSV.\n","Claves: ['radio', 'n_puntos', 'Exactitud', 'Sensibilidad', 'Especificidad', 'Precisión', 'F1-Score', 'AUC']\n","Valores: [2, 16, 72.43589743589743, 92.82051282051282, 38.46153846153847, 71.54150197628458, 0.8080357142857143, 0.843600701293009]\n","Datos guardados en el archivo CSV.\n","Claves: ['radio', 'n_puntos', 'Exactitud', 'Sensibilidad', 'Especificidad', 'Precisión', 'F1-Score', 'AUC']\n","Valores: [2, 18, 69.07051282051282, 92.3076923076923, 30.34188034188034, 68.83365200764818, 0.7886089813800657, 0.8004273504273504]\n"]}],"source":["# Importar las bibliotecas necesarias\n","from sklearn import svm, metrics\n","from skimage.feature import local_binary_pattern\n","import numpy as np\n","import cv2\n","from os import listdir, SEEK_END\n","from os.path import isfile, join\n","from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score\n","import csv\n","import concurrent.futures\n","\n","# Definir la ruta de la carpeta con las imágenes\n","path = '../data'\n","\n","# Categorías\n","categorias = ['NORMAL', 'PNEUMONIA']\n","\n","# Lista de hiperparámetros HOG para probar\n","combinaciones_lbp = [\n","    {'radio': 1, 'n_puntos': 8 * 1},\n","    {'radio': 2, 'n_puntos': 8 * 2},\n","    {'radio': 3, 'n_puntos': 8 * 3},\n","    {'radio': 4, 'n_puntos': 8 * 4},\n","    {'radio': 5, 'n_puntos': 8 * 5},\n","]\n","\n","total_data = []\n","\n","def procesar_imagen(imagen_path, lbp_params, size, etiqueta):\n","    imagen = cv2.imread(imagen_path, cv2.IMREAD_GRAYSCALE)\n","    if imagen is None:\n","        print(f'No se pudo leer la imagen: {imagen_path}')\n","        return None, None\n","\n","    imagen = cv2.resize(imagen, size)\n","\n","    # Poner bien\n","    lbp = local_binary_pattern(imagen, lbp_params['n_puntos'], lbp_params['radio'], method='uniform')\n","    (hist, _) = np.histogram(lbp.ravel(),\n","                             bins=np.arange(0, lbp_params['n_puntos'] + 3),\n","                             range=(0, lbp_params['n_puntos'] + 2))\n","    hist = hist.astype(\"float\")\n","    hist /= (hist.sum() + 1e-7)\n","\n","    return hist, etiqueta\n","\n","def cargar_datos(ruta, lbp_params, size=(128, 128)):\n","    datos = []\n","    etiquetas = []\n","    with concurrent.futures.ThreadPoolExecutor() as executor:\n","        futures = []\n","        for i, cat in enumerate(categorias):\n","            carpeta = join(ruta, cat)\n","            archivos = [f for f in listdir(carpeta) if isfile(join(carpeta, f))]\n","            for archivo in archivos:\n","                imagen_path = join(carpeta, archivo)\n","                futures.append(executor.submit(procesar_imagen, imagen_path, lbp_params, size, i))\n","\n","        for future in concurrent.futures.as_completed(futures):\n","            fd, label = future.result()\n","            if fd is not None:\n","                datos.append(fd)\n","                etiquetas.append(label)\n","\n","    if len(datos) == 0 or len(etiquetas) == 0:\n","        raise ValueError(\"No se pudieron cargar los datos. Asegúrate de que la ruta y las categorías son correctas.\")\n","\n","    return np.array(datos), np.array(etiquetas)\n","\n","\n","for lbp_params in combinaciones_lbp:\n","  # Cargar datos de entrenamiento, validación y prueba\n","  datos_entrenamiento, etiquetas_entrenamiento = cargar_datos(join(path, 'train'), lbp_params)\n","  datos_prueba, etiquetas_prueba = cargar_datos(join(path, 'test'), lbp_params)\n","\n","  # Asegurarse de que hay datos para al menos dos clases\n","  if len(np.unique(etiquetas_entrenamiento)) < 2 or len(np.unique(etiquetas_prueba)) < 2:\n","      raise ValueError(\"Se debe cargar datos para al menos dos clases en cada conjunto de datos.\")\n","\n","  # Crear y entrenar el modelo SVM\n","  modelo = svm.SVC(random_state=42)\n","  modelo.fit(datos_entrenamiento, etiquetas_entrenamiento)\n","\n","  # Evaluar el modelo\n","  etiquetas_predichas = modelo.predict(datos_prueba)\n","  exactitud = metrics.accuracy_score(etiquetas_prueba, etiquetas_predichas)\n","\n","  # Obtener la matriz de confusión\n","  cm = confusion_matrix(etiquetas_prueba, etiquetas_predichas)\n","\n","  # Calcular Sensibilidad (Recall) y Especificidad\n","  sensibilidad = recall_score(etiquetas_prueba, etiquetas_predichas)\n","  especificidad = cm[0,0] / (cm[0,0] + cm[0,1])\n","\n","  # Calcular Precisión y F1-Score\n","  precision = precision_score(etiquetas_prueba, etiquetas_predichas)\n","  f1 = f1_score(etiquetas_prueba, etiquetas_predichas)\n","\n","  # Calcular AUC\n","  # AUC requiere las puntuaciones o probabilidades de clase positiva, no las etiquetas predichas.\n","  # Asegúrate de que tu modelo soporte `decision_function` o `predict_proba`.\n","  if hasattr(modelo, \"decision_function\"):\n","      scores = modelo.decision_function(datos_prueba)\n","  else:\n","      scores = modelo.predict_proba(datos_prueba)[:, 1]\n","  auc = roc_auc_score(etiquetas_prueba, scores)\n","\n","\n","  current_data = {\n","    'Exactitud': exactitud * 100,\n","    'Sensibilidad': sensibilidad * 100,\n","    'Especificidad': especificidad * 100,\n","    'Precisión': precision * 100,\n","    'F1-Score': f1,\n","    'AUC': auc\n","  }\n","\n","  total_data.append(current_data)\n","\n","  #nombre_archivo_csv = '/content/drive/MyDrive/Estudios/Máster IIR/Segundo cuatrimestre/Proyecto VAI-AAI/first_version/hog.csv'\n","  nombre_archivo_csv = \"../results/lbp_combination_svm.csv\"\n","\n","\n","  with open(nombre_archivo_csv, 'a+', newline='') as archivo:\n","      es_vacio = archivo.tell() == 0\n","      escritor = csv.writer(archivo)\n","\n","      # Suponiendo que es_vacio indica si debemos escribir los encabezados\n","      if es_vacio:\n","          # Escribir los nombres de las claves del diccionario y luego los nombres de las métricas\n","          escritor.writerow(list(lbp_params.keys()) + list(current_data.keys()))\n","\n","      # Escribir los valores del diccionario y luego los valores de las métricas\n","      escritor.writerow(list(lbp_params.values()) + list(current_data.values()))\n","\n","  print(\"Datos guardados en el archivo CSV.\")\n","  print(\"Claves:\", list(lbp_params.keys()) + list(current_data.keys()))\n","  print(\"Valores:\", list(lbp_params.values()) + list(current_data.values()))\n","\n","  if len(total_data) == 3:\n","    if len(total_data) == len(combinaciones_lbp):\n","      # Encuentra el índice del mejor F1-Score\n","      index_mejor_f1 = max(range(len(total_data)), key=lambda i: total_data[i]['F1-Score'])\n","\n","      # Encuentra la configuración HOG correspondiente al mejor F1-Score\n","      lbp_correspondiente = combinaciones_lbp[index_mejor_f1]\n","\n","      for i in range(6, 10):\n","        data = {\n","          'radio': lbp_correspondiente['radio'],\n","          'n_puntos': i * lbp_correspondiente['radio'],\n","        }\n","        combinaciones_lbp.append(data)\n","\n"]}],"metadata":{"colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"}},"nbformat":4,"nbformat_minor":0}
